'''
This function processes a RDS enhanced monitoring DATA_MESSAGE, coming from CloudWatch Logs,
and sends the data to a specified Graphite instance via the Carbon protocol.

This is based on code to send these stats to DataDog, via:
  https://github.com/DataDog/dd-aws-lambda-functions

  1. Create a KMS key - http://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html.
  2. Encrypt the token using the AWS CLI.
     $ aws kms encrypt --key-id alias/<KMS key name> --plaintext '{"api_key":"<dd_api_key>", "app_key":"<dd_app_key>"}'
  3. Copy the base-64 encoded, encrypted key (CiphertextBlob) to the KMS_ENCRYPTED_KEYS variable.
  4. Give your function's role permission for the kms:Decrypt action.
     Example:
       {
         "Version": "2012-10-17",
         "Statement": [
           {
             "Effect": "Allow",
             "Action": [
               "kms:Decrypt"
             ],
             "Resource": [
               "<your KMS key ARN>"
             ]
           }
         ]
       }
'''

import gzip
import json
import re
from StringIO import StringIO
from base64 import b64decode
import time
import graphitesend
import logging

# specify Graphite host
CARBON_SERVER = 'ip-of-graphite-host'
CARBON_PORT = 2003
# This is what you want to prefix the data generated by this script with
CARBON_NAMESPACE = "lambda.rdsenhanced"

# Start logging
lambdalogger = logging.getLogger()
lambdalogger.setLevel(logging.INFO)
#lambdalogger.setLevel(logging.DEBUG)
    
# Connect to Graphite
lambdalogger.info('Connecting to Graphite..')
graphite = graphitesend.init(graphite_server=CARBON_SERVER,system_name='',prefix=CARBON_NAMESPACE)

lambdalogger.info('Lambda function initialized, ready to send metrics.')

def _process_rds_enhanced_monitoring_message(ts, message, account, region):
    instance_id = message["instanceID"]
    host_id = message["instanceResourceID"]
    tags = [
        'dbinstanceidentifier:%s' % instance_id,
        'aws_account:%s' % account,
        'engine:%s' % message["engine"],
    ]

    # metrics generation

    # uptime: "54 days, 1:53:04" to be converted into seconds
    uptime = 0
    uptime_msg = re.split(' days?, ', message["uptime"])  # edge case "1 day 1:53:04"
    if len(uptime_msg) == 2:
        uptime += 24 * 3600 * int(uptime_msg[0])
    uptime_day = uptime_msg[-1].split(':')
    uptime += 3600 * int(uptime_day[0])
    uptime += 60 * int(uptime_day[1])
    uptime += int(uptime_day[2])
    stats.add(instance_id,
        'uptime', uptime, timestamp=ts, tags=tags, host=host_id
    )

    stats.add(instance_id,
        'virtual_cpus', message["numVCPUs"], timestamp=ts, tags=tags, host=host_id
    )

    stats.add(instance_id,
        'load.1', message["loadAverageMinute"]["one"],
        timestamp=ts, tags=tags, host=host_id
    )
    stats.add(instance_id,
        'load.5', message["loadAverageMinute"]["five"],
        timestamp=ts, tags=tags, host=host_id
    )
    stats.add(instance_id,
        'load.15', message["loadAverageMinute"]["fifteen"],
        timestamp=ts, tags=tags, host=host_id
    )

    for namespace in ["cpuUtilization", "memory", "tasks", "swap"]:
        for key, value in message[namespace].iteritems():
            stats.add(instance_id,
                '%s.%s' % (namespace.lower(), key), value,
                timestamp=ts, tags=tags, host=host_id
            )

    for network_stats in message["network"]:
        network_interface = network_stats.pop("interface")
        network_tag = ["interface:%s" % network_interface]
        for key, value in network_stats.iteritems():
            stats.add(instance_id,
                'network.%s.%s' %(network_interface, key), value,
                timestamp=ts, tags=tags + network_tag, host=host_id
            )

    for disk_stats in message["diskIO"]:
        disk_device = disk_stats.pop("device")
        disk_tag = ["device:%s" % disk_device]
        for key, value in disk_stats.iteritems():
            stats.add(instance_id,
                'diskio.%s.%s' %(disk_device, key), value,
                timestamp=ts, tags=tags + disk_tag, host=host_id
            )

    for fs_stats in message["fileSys"]:
        fs_name = fs_stats.pop("name")
        fs_mountpoint = fs_stats.pop("mountPoint")
        fs_tag = [
            "name:%s" % fs_name,
            "mountPoint:%s" % fs_mountpoint
        ]
        for key, value in fs_stats.iteritems():
            stats.add(instance_id,
                'filesystem.%s.%s' %(fs_name,key), value,
                timestamp=ts, tags=tags + fs_tag, host=host_id
            )

    for process_stats in message["processList"]:
        process_name = process_stats.pop("name")
        process_id = process_stats.pop("id")
        process_tag = [
            "name:%s" % process_name,
            "id:%s" % process_id
        ]
        # This will end up creating a ton of metrics in Graphite, as process IDs are constantly changing.
        # TODO: Figure out a way to deal with this.. I'm thinking of just combining the process names that are the same
        # together, and including a process count with that. Otherwise, just forget it, and if you need detailed process
        # stats, use the AWS-provided dashboard?
        #for key, value in process_stats.iteritems():
            #stats.add(instance_id,
            #    'process.%s.%s.%s' %(process_name, process_id, key), value,
            #    timestamp=ts, tags=tags + process_tag, host=host_id
            #)


def lambda_handler(event, context):
    ''' Process a RDS enhenced monitoring DATA_MESSAGE,
        coming from CLOUDWATCH LOGS
    '''
    # event is a dict containing a base64 string gzipped
    #print "Incoming event: " + json.dumps(event)

    event = event['awslogs']['data']
    event = json.loads(
        gzip.GzipFile(fileobj=StringIO(event.decode('base64'))).read()
    )

    #print "Decoded event: " + json.dumps(event)
    
    account = event['owner']
    region = context.invoked_function_arn.split(':', 4)[3]

    log_events = event['logEvents']

    for log_event in log_events:
        # The actual log events are escaped json - so we need to load that as json separately from the main load above.
        message = json.loads(log_event['message'])
        ts = log_event['timestamp'] / 1000
        _process_rds_enhanced_monitoring_message(ts, message, account, region)

    return {'Status': 'OK'}


# Helpers to send data to Datadog, inspired from https://github.com/DataDog/datadogpy

class Stats(object):

    def __init__(self):
        self.series = []

    def add(self, instance_id, metric, value, timestamp=None, tags=None, host=None):
        #print "%s: ts %s value %s tags %s host %s" %(metric, timestamp, value, tags, host)
        metricname = instance_id + '.' + metric
       
        # Ensure that the value can be converted to float; if not, proceed without error..
        try:
            float(value)
            results = graphite.send(metricname, value, timestamp)
            lambdalogger.debug(results)
            return True
        except ValueError:
           pass
 
stats = Stats()
